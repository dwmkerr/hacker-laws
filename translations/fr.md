# ğŸ’»ğŸ“– hacker-laws

Lois, thÃ©ories, principes et modÃ¨les que les dÃ©veloppeurs trouveront utiles.

[Traductions](#translations): [ğŸ‡§ğŸ‡·](./translations/pt-BR.md) [ğŸ‡¨ğŸ‡³](https://github.com/nusr/hacker-laws-zh) [ğŸ‡©ğŸ‡ª](./translations/de.md) [ğŸ‡«ğŸ‡·](./translationis/fr.md) [ğŸ‡¬ğŸ‡·](./translations/el.md) [ğŸ‡®ğŸ‡¹](https://github.com/csparpa/hacker-laws-it) [ğŸ‡±ğŸ‡»](./translations/lv.md) [ğŸ‡°ğŸ‡·](https://github.com/codeanddonuts/hacker-laws-kr) [ğŸ‡·ğŸ‡º](https://github.com/solarrust/hacker-laws) [ğŸ‡ªğŸ‡¸](./translations/es-ES.md) [ğŸ‡¹ğŸ‡·](https://github.com/umutphp/hacker-laws-tr)

Vous aimez ce projet ? N'hÃ©sitez pas Ã  [me sponsoriser](https://github.com/sponsors/dwmkerr) ainsi que [les traducteurs](#traductions).

---

<!-- vim-markdown-toc GFM -->

- [Introduction](#introduction)
- [Lois](#lois)
    - [Loi d'Amdahl](#loi-damdahl)
    - [ThÃ©orie de la vitre brisÃ©e](#theorie-de-la-vitre-brisee)
    - [Loi de Brooks](#loi-de-brooks)
    - [Loi de Conway](#loi-de-conway)
    - [Loi de Cunningham](#loi-de-cunningham)
    - [Nombre de Dunbar](#nombre-de-dunbar)
    - [Loi de Gall](#loi-de-gall)
    - [Loi de Goodhart](#loi-de-goodhart)
    - [Rasoir de Hanlon](#rasoir-de-hanlon)
    - [Loi de Hofstadter](#loi-de-hofstadter)
    - [Loi de Hutber](#loi-de-hutber)
    - [Cycle du hype & Loi d'Amara](#cycle-de-hype--loi-damara)
    - [Loi d'Hyrum (loi des interfaces implicites)](#loi-dhyrum)
    - [Loi de Kernighan](#loi-de-kernighan)
    - [Loi de Metcalfe](#loi-de-metcalfe)
    - [Loi de Moore](#loi-de-moore)
    - [Loi de Murphy / Loi de Sod](#loi-de-murphy--loi-de-sod)
    - [Rasoir d'Occam](#rasoir-doccam)
    - [Loi de Parkinson](#loi-de-parkinson)
    - [Effet d'optimisation prÃ©maturÃ©e](#effet-doptimisation-prematuree)
    - [Loi de Putt](#loi-de-putt)
    - [Loi de Reed](#loi-de-reed)
    - [Loi de la conservation de la complexitÃ© (Loi de Tesler)](#loi-de-tesler)
    - [Loi des abstractions qui fuient](#loi-des-abstractions-qui-fuient)
    - [Loi de futilitÃ©](#loi-de-futilite)
    - [Philosophie d'Unix](#philosophie-dunix)
    - [ModÃ¨le de Spotify](#modele-de-spotify)
    - [Loi de Wadler](#loi-de-wadler)
    - [Loi de Wheaton](#loi-de-wheaton)
- [Principes](#principes)
    - [Principe de Dilbert](#principe-de-dilbert)
    - [Principe de Pareto (Regle des 80/20)](#principe-de-pareto-regle-des-8020)
    - [Principe de Peter](#principe-de-peter)
    - [Principe de robustesse (loi de Postel)](#principe-de-robustesse-loi-de-postel)
    - [SOLID](#solid)
    - [Principe de responsabilitÃ© unique](#principe-de-responsabilite-unique)
    - [Principe ouvert/fermÃ©](#principe-ouvertferme)
    - [Principe de substitution de Liskov](#principe-de-substitution-de-liskov)
    - [Principe de sÃ©grÃ©gation des interfaces](#principe-de-segregation-des-interfaces)
    - [Principe d'inversion des dÃ©pendances](#principe-dinversion-des-dependances)
    - [Principe DRY](#principe-dry)
    - [Principe KISS](#principe-kiss)
    - [YAGNI](#yagni)
    - [Illusions de l'informatique distribuÃ©e](#illusions-de-linformatique-distribuee)
- [Ã€ lire](#a-lire)
- [Traductions](#traductions)
- [Projets liÃ©s](#projets-lies)
- [Contribuer](#contribuer)
- [TODO](#todo)

<!-- vim-markdown-toc -->

## Introduction

Il y a beaucoup de "lois" dont les gens parlent quand on discute de dÃ©veloppement. Ce repository offre une vue d'ensemble et une rÃ©fÃ©rence des plus communes. N'hÃ©sitez pas Ã  partager et Ã  proposer vos PRs !

â—: Ce repo ne *prÃ©conise* aucune des lois, principes ou modÃ¨les qui y sont expliquÃ©s. Leur application devrait toujours Ãªtre le sujet d'un dÃ©bat, et dÃ©pend grandement de ce sur quoi vous travaillez.

## Lois

Nous y voila !

### Loi d'Amdahl

[Loi d'Amdahl sur Wikipedia](https://fr.wikipedia.org/wiki/Loi_d%27Amdahl)

> La loi d'Amdahl est une formule qui montre le *gain de vitesse potentiel* sur un calcul, obtenu en augmentant les ressources d'un systÃ¨me. Habituellement utilisÃ© en calcul parallÃ¨le, elle peut prÃ©dire les bÃ©nÃ©fices rÃ©els de l'accroissement du nombre de processeurs. BÃ©nÃ©fices qui sont limitÃ©s par le potentiel du programme Ã  Ãªtre parallÃ©lisÃ©.

Prenons un exemple: si un programme est composÃ© de 2 parties, la partie A devant Ãªtre exÃ©cutÃ© par un seul processeur et la partie B pouvant Ãªtre parallÃ©lisÃ©e, alors on peut constater qu'ajouter plusieurs processeurs au systÃ¨me executant le programme ne peut avoir qu'un bÃ©nÃ©fice limitÃ©. Cela peut potentiellement amÃ©liorer grandement la vitesse de la partie B, mais la vitesse de la partie A restera inchangÃ©e.

Le diagramme ci-dessous montre quelques exemples de gain de vitesse potentiels :

<img width="480px" alt="Diagram: Amdahl's Law" src="../images/amdahls_law.png">

*(Reference: par Daniels220 sur English Wikipedia, Creative Commons Attribution-Share Alike 3.0 Unported, https://en.wikipedia.org/wiki/File:AmdahlsLaw.svg)*

Comme il est visible, un programme qui est parallÃ©lisable Ã  50% ne bÃ©nÃ©ficiera que trÃ¨s peu au delÃ  des 10 processeurs, tandis qu'un programme parallÃ©lisable Ã  95% peut encore gagner en vitesse avec plus d'un millier de processeurs.

Ã€ mesure que la [loi de Moore](#loi-de-moore) ralenti et que l'accÃ©lÃ©ration de la vitesse de calcul des processeurs diminue, la parallÃ©lisation est la clef de l'amÃ©lioration des performances. Prenons par exemple la programmation graphique avec les calculs de Shader: chaque pixel ou fragment peut Ãªtre rendu en parallÃ¨le. Ce qui explique que les cartes graphiques rÃ©centes ont souvent plusieurs milliers de coeurs de calcul (GPUs ou Shader Units).

Voir aussi:

- [Loi de Brooks](#loi-de-brooks)
- [Loi de Moore](#loi-de-moore)

### ThÃ©orie de la vitre brisÃ©e

[ThÃ©orie de la vitre brisÃ©e sur Wikipedia](https://fr.wikipedia.org/wiki/Hypoth%C3%A8se_de_la_vitre_bris%C3%A9e)

La thÃ©orie de la vitre brisÃ©e suggÃ¨re que des signes visibles de criminalitÃ© (ou de manque de soin d'un environnement) amÃ¨ne Ã  des crimes plus nombreux et plus sÃ©rieux (ou une plus grande dÃ©tÃ©rioration de l'environnement).

Cette thÃ©orie est aussi appliquÃ© au dÃ©veloppement logiciel pour suggÃ©rer que du code de mauvaise qualitÃ© (ou de la [dette technique](#TODO)) peut amener Ã  penser que les efforts fait pour amÃ©liorer le code ne sont pas valorisÃ©s, voir complÃ¨tement ignorÃ©s. Menant ainsi Ã  une plus grande dÃ©tÃ©rioration de la qualitÃ© du code au fil du temps.

Voir aussi:

- [Dette technique](#TODO)

Exemples:

- [The Pragmatic Programming: Software Entropy](https://pragprog.com/the-pragmatic-programmer/extracts/software-entropy)
- [Coding Horror: The Broken Window Theory](https://blog.codinghorror.com/the-broken-window-theory/)
- [OpenSource: Joy of Programming - The Broken Window Theory](https://opensourceforu.com/2011/05/joy-of-programming-broken-window-theory/)

### Loi de Brooks

[Loi de Brooks sur Wikipedia](https://fr.wikipedia.org/wiki/Loi_de_Brooks)

> Ajouter des personnes Ã  un projet en retard accroÃ®t son retard.

Cette loi suggÃ¨re que dans beaucoup de cas, tenter d'accÃ©lÃ©rer le bouclage d'un projet qui est en retard en ajoutant plus de personnes dessus rendra le projet encore plus en retard. Brooks est clair sur le fait qu'il s'agit d'une grande simplification, mais le raisonnement gÃ©nÃ©ral est que la vitesse d'avancement du projet sur le court terme diminue Ã  cause du temps nÃ©cessaire Ã  l'intÃ©gration des nouveaux arrivants et du surplus de communication nÃ©cessaire. De plus, de nombreuses tÃ¢ches peuvent ne pas Ãªtre divisibles, comprendre rÃ©parties entre plusieurs personnes. Ce qui abaisse encore le potentiel d'augmentation de la vitesse d'avancement du projet.

La phrase bien connue "Neuf femmes ne peuvent pas faire un bÃ©bÃ© en un mois" illustre la loi de Brooks, en particulier le fait que certaines tÃ¢ches ne sont pas divisibles ou parallÃ©lisables.

C'est un thÃ¨me central du livre '[The Mythical Man Month](#reading-list)'.

Voir aussi:

- [Death March](#todo)
- [Reading List: The Mythical Man Month](#reading-list)

### Loi de Conway

[Loi de Conway sur Wikipedia](https://fr.wikipedia.org/wiki/Loi_de_Conway)

Cette loi suggÃ¨re que les contours techniques d'un systÃ¨me reflÃ¨tent la structure de l'organisation qui a produit le systÃ¨me. Cette loi est souvent Ã©voquÃ©e quand on cherche Ã  amÃ©liorer l'organisation en question. Si une organisation est structurÃ©e en plusieurs unitÃ©s dÃ©connectÃ©es, le logiciel qui est produit le sera aussi. Si une organisation est composÃ©e de silos verticaux orientÃ©s autour de fonctionnalitÃ©s ou services, le logiciel le reflÃ¨tera aussi.

Voir aussi:

- [ModÃ¨le de Spotify](#modele-de-spotify)

### Loi de Cunningham

[Loi de Cunningham sur Wikipedia](https://en.wikipedia.org/wiki/Ward_Cunningham#Cunningham's_Law)

> Le meilleur moyen d'obtenir une bonne rÃ©ponse sur Internet n'est pas de poser la question, mais de poster la mauvaise rÃ©ponse.

Selon Steven McGeady, Ward Cunningham lui aurait conseillÃ© au dÃ©but des annÃ©es 1980: "le meilleur moyen d'obtenir une bonne rÃ©ponse sur Internet n'est pas de poser la question, mais de poster la mauvaise rÃ©ponse." McGeady baptisa cette phrase la loi de Cunningham, bien que Cunningham lui mÃªme en rÃ©fute la parentÃ©. Faisant initialement rÃ©fÃ©rence aux interactions sur Usenet, cette loi a Ã©tÃ© utilisÃ© pour dÃ©crire le fonctionnement d'autres communautÃ©s en ligne (Wikipedia, Reddit, Twitter, Facebook).

Voir aussi:

- [XKCD 386: "Duty Calls"](https://xkcd.com/386/)

### Nombre de Dunbar

[Nombre de Dunbar sur Wikipedia](https://fr.wikipedia.org/wiki/Nombre_de_Dunbar)

"Le nombre de Dunbar est le nombre maximum d'individus avec lesquels une personne peut entretenir simultanÃ©ment une relation humaine stable." Ã€ savoir une relation dans laquelle un individu sait qui est chaque personne et comment elle est liÃ©e aux autres personnes. Il n'y a pas de vÃ©ritable consensus sur le nombre exact. "... [Dunbar] avance que les Ãªtres humains peuvent maintenir confortablement seulement 150 relations stables". Il place le nombre dans un contexte social: "le nombre de personnes envers lesquelles vous ne vous sentiriez pas embarrassÃ© de partager un verre si vous les croisiez par hasard dans un bar". Les estimations du nombre tombent gÃ©nÃ©ralement entre 100 et 250.

De mÃªme que les relations stables entre individus, la relation entre un dÃ©veloppeur et une codebase requiert des efforts pour Ãªtre maintenu. Lorsque nous faisons face Ã  de larges projets compliquÃ©s ou nombreux, nous pouvons nous aider de conventions, de procÃ©dures ou de modÃ¨les. Le nombre de Dunbar est important Ã  garder Ã  l'esprit non seulement lorsque la taille d'une entreprise augmente mais aussi lorsqu'on dÃ©cide de la portÃ©e des efforts Ã  rÃ©aliser pour une Ã©quipe. Pris dans un contexte d'ingÃ©nierie, il reprÃ©sente le nombre de projets sur lesquels on pourrait sereinement faire du support si on y Ã©tait amenÃ©.

Voir aussi :

- [Loi de Conway](#loi-de-conway)

### Loi de Gall

[Loi de Gall sur Wikipedia](https://en.wikipedia.org/wiki/John_Gall_(author)#Gall's_law)

> Un systÃ¨me complexe qui fonctionne est une Ã©volution d'un systÃ¨me simple qui fonctionne. Un systÃ¨me complexe entiÃ¨rement conÃ§u depuis zero ne fonctionne jamais et ne peut pas Ãªtre modifiÃ© pour le faire fonctionner. Il faut recommencer avec un systÃ¨me simple qui fonctionne.
> ([John Gall](https://en.wikipedia.org/wiki/John_Gall_(author)))

La loi de Gall implique que les tentatives de *concevoir* un systÃ¨me fortement complexe ont de grandes chances d'Ã©chouer. Les systÃ¨mes fortement complexes sont rarement construits d'un seul coup, mais Ã©voluent plutÃ´t depuis des systÃ¨mes plus simples.

Un exemple classique est le world-wide-web. Dans son Ã©tat actuel, il s'agit d'un systÃ¨me fortement complexe. Cependant, il Ã©tait initialement dÃ©finit comme un simple moyen d'Ã©changer du contenu entre Ã©tablissements universitaires. Ayant atteint cet objectif avec un grand succÃ¨s, le systÃ¨me a Ã©voluÃ© pour devenir de plus en plus complexe au fil du temps.

Voir aussi :

- [KISS (Keep It Simple, Stupid)](#principe-kiss)

### Loi de Goodhart

[Loi de Goodhart sur Wikipedia](https://fr.wikipedia.org/wiki/Loi_de_Goodhart)

> Toute rÃ©gularitÃ© statistique observÃ©e a tendance Ã  perdre de sa fiabilitÃ© lorsque on tente de la contrÃ´ler.
> *Charles Goodhart*

Souvent aussi Ã©noncÃ©e de cette maniÃ¨re :

> Lorsqu'une mesure devient un objectif, elle cesse d'Ãªtre une bonne mesure.
> *Marilyn Strathern*

Cette loi indique que les optimisations basÃ©es sur une mesure peuvent amener Ã  une perte de valeur de la mesure elle mÃªme. Un ensemble de mesures ([KPIs](https://en.wikipedia.org/wiki/Performance_indicator)) trop restraint appliquÃ© aveuglÃ©ment Ã  un process dÃ©forme le rÃ©sultat. Les gens tendent Ã  "tricher" localement pour satisfaire une mesure en particulier sans faire attention aux effect globaux de leurs actions sur le systÃ¨me.

Exemples concrets :

- Il est possible d'atteindre un taux de couverture du code arbitraire en rÃ©digeant des tests qui ne vÃ©rifient rien. Alors que le but initial de la mesure Ã©tait d'avoir du code correctement testÃ©.
- Mesurer les performances des dÃ©veloppeurs avec le nombre de lignes de code rÃ©digÃ©es amÃ¨ne Ã  des codebases inutilement grosses.

Voir aussi :

- [Goodhartâ€™s Law: How Measuring The Wrong Things Drive Immoral Behaviour](https://coffeeandjunk.com/goodharts-campbells-law/)
- [Dilbert on bug-free software](https://dilbert.com/strip/1995-11-13)

### Rasoir de Hanlon

[Rasoir de Hanlon sur Wikipedia](https://fr.wikipedia.org/wiki/Rasoir_de_Hanlon)

> Ne jamais attribuer Ã  la malveillance ce que la bÃªtise suffit Ã  expliquer.
> Robert J. Hanlon

Ce principe suggÃ¨re que des actions produisant un mauvais rÃ©sultat ne sont pas toujours motivÃ©es par de mauvaises intentions. Il est au contraire plus probable que le problÃ¨me se situe dans la comprÃ©hension de ces actions et de leurs impacts.

### Loi de Hofstadter

[Loi de Hofstadter sur Wikipedia](https://fr.wikipedia.org/wiki/Loi_de_Hofstadter)

> Il faut toujours plus de temps que prÃ©vu, mÃªme en tenant compte de la loi de Hofstadter.
> (Douglas Hofstadter)

Vous pourrez entendre parler de cette loi lorsqu'on cherche Ã  estimer le temps nÃ©cessaire pour faire quelque chose. C'est un lieu commun de dire que nous ne sommes pas trÃ¨s bon pour estimer le temps nÃ©cessaire Ã  boucler un projet en dÃ©veloppement logiciel.

c'est un extrait du livre '[GÃ¶del, Escher, Bach: An Eternal Golden Braid](#a-lire)'.

Voir aussi :

- [Ã€ lire: GÃ¶del, Escher, Bach: An Eternal Golden Braid](#a-lire)

### Loi de Hutber

[Loi de Hutber sur Wikipedia](https://en.wikipedia.org/wiki/Hutber%27s_law)

> AmÃ©lioration veut dire dÃ©tÃ©rioration.
> ([Patrick Hutber](https://en.wikipedia.org/wiki/Patrick_Hutber))

Cette loi suggÃ¨re que les amÃ©liorations apportÃ©es Ã  un systÃ¨me vont mener Ã  la dÃ©tÃ©rioration d'autres choses. Ou qu'elles vont cacher d'autres dÃ©tÃ©riorations, amenant globalement Ã  une dÃ©gradation de l'Ã©tat du systÃ¨me.

Par exemple, un abaissement de la latence de rÃ©ponse sur une route (end-point) peut causer des problÃ¨mes de dÃ©bit et de capacitÃ© plus loin, affectant un sous-systÃ¨me entiÃ¨rement diffÃ©rent.

### Cycle du hype & Loi d'Amara

[Cycle du hype sur Wikipedia](https://fr.wikipedia.org/wiki/Cycle_du_hype)

> On a tendance Ã  surestimer l'effet d'une technologie sur le court terme et Ã  le surestimer sur le long terme.
> (Roy Amara)

Le cycle du hype est une reprÃ©sentation visuelle de l'attrait et du dÃ©veloppement d'une technologie au fil du temps. Initialement rÃ©alisÃ© par Gartner, le concept est plus clair avec un diagramme :

![The Hype Cycle](../images/gartner_hype_cycle.png)

*(Reference: par Jeremykemp sur English Wikipedia, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=10547051)*

En clair, ce cycle montre qu'il y a gÃ©nÃ©ralement un pic d'excitation concernant les nouvelles technologies et leur potentiel impact. Les Ã©quipes adoptent ces technologies rapidement et se retrouvent parfois dÃ©Ã§ues des rÃ©sultats. Cela peut Ãªtre Ã  cause d'un manque de maturitÃ© de la technologie, ou parce que les applications concrÃ¨tes de cette technologie ne sont pas encore totalement maitrisÃ©es. AprÃ¨s un certain temps, les opportunitÃ©s d'utiliser cette technologie ainsi que ses capacitÃ©s augmentent suffisamment pour que les Ã©quipes deviennent vraiment productives. La citation de Roy Amara le rÃ©sume de maniÃ¨re plus succincte: "On a tendance Ã  surestimer l'effet d'une technologie sur le court terme et Ã  le surestimer sur le long terme".

### Loi d'Hyrum (loi des interfaces implicites)

[Loi d'Hyrum en ligne](http://www.hyrumslaw.com/)

> > PassÃ© un certain nombre d'utilisateur d'une API, peu importe ce qui est promis par l'interface, tous les comportements possibles du systÃ¨me seront utilisÃ©s.
> (Hyrum Wright)

La loi d'Hyrum dÃ©cris le fait que lorsqu'une API a un *nombre suffisamment Ã©levÃ© d'utilisateurs*, tous les comportements de l'API (y compris ceux qui ne sont pas dÃ©finis publiquement) seront utilisÃ©s par quelqu'un. Un exemple trivial peut concerner les Ã©lÃ©ments non fonctionnels de l'API comme le temps de rÃ©ponse. Un exemple plus subtil peut Ãªtre l'utilisation d'une regex sur les messages d'erreurs pour en dÃ©terminer le *type*. MÃªme si la spÃ©cification de l'API ne mentionne rien quant au contenu des messages, *certains* utilisateurs peuvent utiliser ces messages. Un changement au niveau de ces messages reviendrait Ã  casser l'API pour ces utilisateurs.

Voir aussi :

- [Loi des abstractions qui fuient](#loi-des-abstractions-qui-fuient)
- [XKCD 1172](https://xkcd.com/1172/)

### Loi de Kernighan

> Debugger est deux fois plus difficile que de rÃ©diger le code initial. Par consÃ©quent si vous rÃ©diger le code de maniÃ¨re aussi maligne que possible, vous n'Ãªtes, par dÃ©finition, pas assez intelligent pour le debugger.
> (Brian Kernighan)

La loi de Kernighan est nommÃ©e d'aprÃ¨s [Brian Kernighan](https://en.wikipedia.org/wiki/Brian_Kernighan) et est basÃ©e d'une citation du livre de Kernighan et Plauger: [The Elements of Programming Style](https://en.wikipedia.org/wiki/The_Elements_of_Programming_Style).

> Tout le monde sait que debugger est 2 fois plus difficile que de rÃ©diger le programme en premier lieu. Donc si vous Ãªtes aussi malin que possible en le rÃ©digeant, comment pourrez vous le debugger ?

Bien qu'Ã©tant hyperbolique, la loi de Kernighan prÃ©sente l'argument que du code simple est prÃ©fÃ©rable Ã  du code complexe, car tout problÃ¨me qui pourrait apparaitre dans du code complexe sera couteux voir impossible Ã  debugger.

Voir aussi :

- [Principe KISS](#principe-kiss)
- [Philosophie d'Unix](#philosophie-dunix)
- [Rasoir d'Occam](#rasoir-doccam)

### Loi de Metcalfe

[Loi de Metcalfe sur Wikipedia](https://fr.wikipedia.org/wiki/Loi_de_Metcalfe)

> Lâ€™utilitÃ© dâ€™un rÃ©seau est proportionnelle au carrÃ© du nombre de ses utilisateurs.

Cette loi est basÃ©e sur le nombre de connexions par pair Ã  l'intÃ©rieur d'un systÃ¨me et est fortement liÃ©e Ã  la [Loi de Reed](#loi-de-reed). Odlyzko et d'autres ont soutenus l'argument que la loi de Reed et la loi de Metcalfe surestiment la valeur du systÃ¨me en ne tenant pas compte des limites de l'intellect humain. Voir le [Nombre de Dunbar](#nombre-de-dunbar).

Voir aussi :

- [Loi de Reed](#loi-de-reed)
- [Nombre de Dunbar](#nombre-de-dunbar)

### Loi de Moore

[Loi de Moore sur Wikipedia](https://en.wikipedia.org/wiki/Moore%27s_law)

> Le nombre de transistors dans un circuit intÃ©grÃ© double approximativement tous les 2 ans.

Souvent utilisÃ©e pour illustrer la grande vitesse Ã  laquelle les semi-conducteurs et les technologies de puces informatiques ont Ã©voluÃ©es. Cette prÃ©diction de Moore s'est rÃ©vÃ©lÃ©e Ãªtre trÃ¨s prÃ©cise des annÃ©es 70 aux annÃ©es 2000. Plus rÃ©cemment ceci dit, la tendance a ralentie, en partie du [aux limites physiques de la miniaturisation des composants](https://en.wikipedia.org/wiki/Quantum_tunnelling). Cependant, les avancÃ©es dans la parallÃ©lisation et les changements potentiellement rÃ©volutionnaires dans les technologies des semi-conducteurs et du calcul quantique continueront peut Ãªtre de faire respecter la loi de Moore pour les dÃ©cennies Ã  venir.

### Loi de Murphy / Loi de Sod

[Loi de Murphy sur Wikipedia](https://fr.wikipedia.org/wiki/Loi_de_Murphy)

> Tout ce qui est susceptible d'aller mal, ira mal.

Ã‰noncÃ©e par [Edward A. Murphy, Jr](https://en.wikipedia.org/wiki/Edward_A._Murphy_Jr.), la *loi de Murphy* dÃ©clare que si quelque chose peut mal tourner, cela tournera mal.

C'est une formule bien connue des dÃ©veloppeurs. Parfois l'inattendu surviens lors du dÃ©veloppement, des tests, ou mÃªme en production. Cette loi peut aussi Ãªtre liÃ©e Ã  la *loi de Sod* (plus courante en Anglais d'Angleterre) :

> Si quelque chose peut mal tourner, cela tournera mal. Au pire moment possible.

Ces 'lois' sont souvent utilisÃ©es dans un sens humoristique. Cependant, des biais cognitifs tels que le [*biais de confirmation*](#TODO) et le [*biais de sÃ©lection*](#TODO) peuvent amener des gens Ã  porter trop d'importance Ã  ces lois. (On ne porte pas attention aux choses quand elles fonctionnent, la plupart du temps. Quand il y a un problÃ¨me en revanche, c'est plus remarquÃ© et peut entrainer des discussions)

Voir aussi :

- [Biais de confirmation](#TODO)
- [Biais de sÃ©lection](#TODO)

### Rasoir d'Occam

[Rasoir d'Occam sur Wikipedia](https://fr.wikipedia.org/wiki/Rasoir_d%27Ockham)

> Les multiples ne doivent pas Ãªtre utilisÃ©s sans nÃ©cessitÃ©.
> William of Ockham

Le rasoir d'Occam nous indique que parmi plusieurs solutions possibles, la plus probable est celle Ã  laquelle est attachÃ©e le moins de concepts et d'Ã  priori. Cette solution est la plus simple et rÃ©sout le problÃ¨me donnÃ© sans ajouter accidentellement de la complexitÃ© et de potentielles consÃ©quences nÃ©gatives.

Voir aussi :

- [YAGNI](#yagni)
- [No Silver Bullet: Accidental Complexity and Essential Complexity](https://en.wikipedia.org/wiki/No_Silver_Bullet)

Exemple :

- [Lean Software Development: Eliminate Waste](https://en.wikipedia.org/wiki/Lean_software_development#Eliminate_waste)

### Loi de Parkinson

[Loi de Parkinson sur Wikipedia](https://fr.wikipedia.org/wiki/Loi_de_Parkinson)

> Le travail sâ€™Ã©tale de faÃ§on Ã  occuper le temps disponible pour son achÃ¨vement.

Dans son contexte original, cette loi Ã©tait basÃ©e sur l'Ã©tude des administrations. Elle peut Ãªtre appliquÃ©e aux projets de dÃ©veloppement logiciel, la thÃ©orie Ã©tant que les Ã©quipes seront inefficaces jusqu'Ã  l'approche des deadlines puis se dÃ©pÃªcheront de finir le travail pour tenir les dÃ©lais. Rendant la deadline plus ou moins arbitraire.

Si cette loi est combinÃ©e avec la [loi de Hofstadter](#loi-de-hofstadter), on arrive Ã  une perspective encore plus pessimiste: le travail s'Ã©tale pour occuper tout le temps disponible et au final prendra *encore plus de temps que prÃ©vu*.

Voir aussi :

- [Loi de Hofstadter](#loi-de-hofstadter)

### Effet d'optimisation prÃ©maturÃ©e

[Optimisation prÃ©maturÃ©e sur WikiWikiWeb](http://wiki.c2.com/?PrematureOptimization)

> L'optimisation prÃ©maturÃ©e est la source de tous les maux.
> [(Donald Knuth)](https://twitter.com/realdonaldknuth?lang=en)

Dans l'article [Structured Programming With Go To Statements](http://wiki.c2.com/?StructuredProgrammingWithGoToStatements) rÃ©digÃ© par Donald Knuth, celui-ci Ã©crit : "Les programmeurs perdent un temps Ã©norme Ã  rÃ©flÃ©chir ou Ã  se soucier de la vitesse de certaines parties non-critiques de leurs programmes. Et ces tentatives d'Ãªtre performant ont en vÃ©ritÃ© un impact fortement nÃ©gatif quand on prend en compte le debugging et la maintenance. Nous devrions oublier les petits rendements. Disons que 97% du temps: **l'optimisation prÃ©maturÃ©e est la source de tous les maux**. Ceci dit, nous ne devrions pas louper les opportunitÃ©s disponibles dans ces 3% cruciaux."

*L'optimisation prÃ©maturÃ©e* peut aussi Ãªtre dÃ©finie plus simplement comme: optimiser avant qu'on soit sÃ»r qu'il faille le faire.

### Loi de Putt

[Loi de Putt sur Wikipedia](https://en.wikipedia.org/wiki/Putt%27s_Law_and_the_Successful_Technocrat)

> La technologie est dominÃ©e par deux types de personnes: celles qui comprennent ce qu'elles ne managent pas et celles qui managent ce qu'elles ne comprennent pas.

La loi de Putt est souvent suivie par sa corollaire :

> Toute hiÃ©rarchie technique dÃ©veloppe tÃ´t ou tard une inversion de compÃ©tence.

Ces dÃ©clarations suggÃ¨rent que Ã©tant donnÃ© les divers critÃ¨res de sÃ©lection et tendances dans la maniÃ¨re dont les groupes s'organisent, on trouvera au sein d'une entreprise technique deux types d'employÃ©s: des employÃ©s compÃ©tents techniquement non cadres et des employÃ©s Ã  des postes de gestion qui ne comprennent pas aussi bien la complexitÃ© et les difficultÃ©s techniques. Cela peut Ãªtre attribuÃ© Ã  des phÃ©nomÃ¨nes comme le [principe de Peter](#principe-de-peter) or le [principe de Dilbert](#principe-de-dilbert).

Ceci dit, il est important de prÃ©ciser que ce genre de lois sont des gÃ©nÃ©ralisations et s'appliquent Ã  *certains* types d'organisation, sans s'appliquer Ã  d'autres.

Voir aussi :

- [Principe de Peter](#principe-de-peter)
- [Principe de Dilbert](#principe-de-dilbert)

### Loi de Reed

[Loi de Reed sur Wikipedia](https://fr.wikipedia.org/wiki/Loi_de_Reed)

> L'utilitÃ© des grands rÃ©seaux, particuliÃ¨rement des rÃ©seaux sociaux, s'accroit exponentiellement avec la taille du rÃ©seau.

Cette loi est basÃ©e sur la thÃ©orie des graphs, oÃ¹ l'utilitÃ© s'accroit avec le nombre de sous-groupes possibles. Odlyzko et d'autres ont avancÃ© l'argument que la loi de Reed surestime l'utilitÃ© du rÃ©seau en ne prenant pas en compte les limites du cerveau humain; voir le [nombre de Dunbar](#nombre-de-dunbar).

Voir aussi :

- [Loi de Metcalfe](#loi-de-metcalfe)
- [Nombre de Dunbar](#nombre-de-dunbar)

### Loi de la conservation de la complexitÃ© (Loi de Tesler)

[Loi de la conservation de la complexitÃ© sur Wikipedia](https://en.wikipedia.org/wiki/Law_of_conservation_of_complexity)

Cette loi Ã©nonce qu'il y a une certaine quantitÃ© de complexitÃ© dans un systÃ¨me qui ne peut pas Ãªtre rÃ©duite.

Une partie de la complexitÃ© d'un systÃ¨me est du Ã  de la nÃ©gligence. ConsÃ©quence d'une mauvaise structure, d'erreurs ou d'une mauvaise modÃ©lisation du problÃ¨me Ã  rÃ©soudre. Ce type de complexitÃ© peut Ãªtre rÃ©duit, voir Ã©liminÃ©. Cependant, il y a une autre partie de la complexitÃ© qui est intrinsÃ¨que, du au problÃ¨me qu'on cherche Ã  rÃ©soudre. Ce type de complexitÃ© peut Ãªtre dÃ©placÃ© mais pas Ã©liminÃ©.

Un Ã©lÃ©ment interessant soulevÃ© par cette loi est la suggestion que mÃªme en simplifiant le systÃ¨me en entier, la complexitÃ© intrinsÃ¨que n'est pas rÃ©duite, elle est *dÃ©portÃ©e sur l'utilisateur*, qui doit alors compenser.

### Loi des abstractions qui fuient

[Loi des abstractions qui fuient sur Joel on Software](https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/)

> Toutes les abstractions non-triviales fuient plus ou moins.
> ([Joel Spolsky](https://twitter.com/spolsky))

Cette loi Ã©nonce que les abstractions, qui sont gÃ©nÃ©ralement utilisÃ© en informatique pour simplifier l'utilisation de systÃ¨mes complexes, vont "fuirent" une partie du systÃ¨me sous-jacent dans certaines situations.

Si on prends l'exemple de la lecture d'un fichier. Les APIs pour les systÃ¨mes de fichier sont une *abstraction* des systÃ¨mes plus bas niveau du kernel, qui sont eux mÃªme une abstraction du processus physique de changement de donnÃ©es sur le disque (ou la mÃ©moire flash pour un SSD). Dans la plupart des cas, l'abstraction consistant Ã  traiter un fichier comme un flux de donnÃ©es binaire fonctionnera comme prÃ©vu. Cependant, avec un disque magnÃ©tique la lecture de donnÃ©es sÃ©quentielle sera *significativement* plus rapide que la lecture de donnÃ©es alÃ©atoire (due aux couts plus Ã©levÃ©s d'erreurs de page). Mais pour un disque SSD, ces couts supplÃ©mentaires n'existent pas. On peut donc voir que les dÃ©tails sous-jacents doivent Ãªtre compris pour gÃ©rer cet exemple efficacement (par exemple les fichiers d'index de base de donnÃ©es sont structurÃ©s de maniÃ¨re Ã  limiter le surcout des accÃ¨s alÃ©atoires). L'abstraction "fuit" certains dÃ©tails d'implÃ©mentation que le dÃ©veloppeur peut donc avoir besoin de connaitre.

L'exemple ci-dessus peut devenir plus complexe quand des abstractions *supplÃ©mentaires* sont prÃ©sentes. Par exemple, le systÃ¨me d'exploitation Linux permet aux fichiers d'accÃ©der Ã  des fichiers via un rÃ©seau, mais les prÃ©sente sur la machine comme Ã©tant "normaux". Cette abstraction va fuir s'il y a des problÃ¨mes de rÃ©seau. Si un dÃ©veloppeur traite ces fichiers comme Ã©tant "normaux" sans considÃ©rer le fait qu'ils peuvent Ãªtre sujets Ã  de la latence ou des Ã©checs rÃ©seaux, le logiciel fonctionnera mal.

L'article dÃ©crivant cette loi suggÃ¨re qu'une dÃ©pendance trop forte aux abstractions combinÃ©e Ã  une faible comprÃ©hension des processus sous-jacent rend le problÃ¨me *plus* complexe Ã  gÃ©rer dans certains cas.

Voir aussi :

- [Loi d'Hyrum](#loi-dhyrum)

Exemples concrets :

- - [DÃ©marrage lent de Photoshop](https://forums.adobe.com/thread/376152) - un problÃ¨me que j'ai eut par le passÃ©. Photoshop Ã©tait lent au dÃ©marrage, prenant parfois plusieurs minutes. Le problÃ¨me venait du fait que le logiciel rÃ©cupÃ©rait des informations sur l'imprimante par dÃ©faut au dÃ©marrage. Hors, si cette imprimante Ã©tait reliÃ©e par rÃ©seau, cela prenait extrÃªmement longtemps. *L'abstraction* de l'imprimante rÃ©seau prÃ©sentÃ©e comme Ã©tant similaire Ã  une imprimante locale causait ce problÃ¨me pour les utilisateurs avec une mauvaise connexion.

### Loi de futilitÃ©

[Loi de futilitÃ© sur Wikipedia](https://fr.wikipedia.org/wiki/Loi_de_futilit%C3%A9_de_Parkinson)

Cette loi suggÃ¨re que les organisations donnent largement plus de temps et d'attention Ã  des dÃ©tails triviaux ou cosmÃ©tiques qu'aux problÃ¨mes fondamentaux et difficiles.

L'exemple fictif couramment utilisÃ© est celui d'un comitÃ© approuvant les plans d'une centrale nuclÃ©aire et qui passe la majoritÃ© de son temps Ã  parler du local Ã  vÃ©lo plutÃ´t que de la conception de la centrale en elle mÃªme. Il peut Ãªtre difficile de participer de maniÃ¨re utile Ã  des discussions concernant des sujets vastes et complexes sans une grande expertise ou prÃ©paration. Cependant les gens veulent Ãªtre vu comme participant de maniÃ¨re utile. D'oÃ¹ une tendance Ã  trop se focaliser sur des dÃ©tails qui peuvent Ãªtre abordÃ©s simplement mais qui n'ont pas particuliÃ¨rement d'importance.

L'exemple ci-dessus Ã  conduit Ã  l'utilisation du terme 'Bike Shedding' (en rapport Ã  l'abri Ã  vÃ©lo) comme expression dÃ©signant une perte de temps sur des dÃ©tails triviaux. Un autre terme apparentÃ© est '[Yak Shaving](https://en.wiktionary.org/wiki/yak_shaving)', qui dÃ©signe une activitÃ© apparemment inutile qui fait partie d'une longe chaine de prÃ©-requis Ã  la tÃ¢che principale.

### Philosophie d'Unix

[The Unix Philosophie d'Unix sur Wikipedia](https://fr.wikipedia.org/wiki/Philosophie_d%27Unix)

La philosophie d'Unix consiste Ã  dire que les programmes informatiques doivent Ãªtre petits, ne faire qu'une seule chose et la faire bien. Cela peut rendre plus simple la construction de systÃ¨mes en combinant des unitÃ©s simples petites et bien dÃ©finies plutÃ´t que des programmes larges, complexes et servant Ã  plusieurs choses.

Certaines pratiques rÃ©centes comme l'architecture en microservices peut Ãªtre vue comme une application de cette loi, oÃ¹ les services sont petits et ne font qu'une seule chose, permettant la crÃ©ation de comportements complexes Ã  partir de briques qui sont simples.

### ModÃ¨le de Spotify

[ModÃ¨le de Spotify sur Spotify Labs](https://labs.spotify.com/2014/03/27/spotify-engineering-culture-part-1/)

Le modÃ¨le de Spotify est une approche Ã  la structure d'entreprise et des Ã©quipes qui a Ã©tÃ© popularisÃ©e par Spotify. Dans ce modÃ¨le, les Ã©quipes sont organisÃ©es autour des fonctionnalitÃ©s plutÃ´t que des technologies.

Le modÃ¨le de Spotify a Ã©galement popularisÃ© les concepts de Tribus, Guildes, et Chapitres qui sont d'autres Ã©lÃ©ments de leur structure.

### Loi de Wadler

[Loi de Wadler sur wiki.haskell.org](https://wiki.haskell.org/Wadler's_Law)

> Dans toute conception de langage, le temps total passÃ© Ã  discuter un aspect de cette liste est proportionnel Ã  deux Ã©levÃ© Ã  la puissance de la position correspondante.
> 1. SÃ©mantique
> 2. Syntaxe
> 3. Syntaxe lexicale
> 4. Syntaxe lexicale des commentaires
> (en clair, pour chaque heure passÃ©e sur la sÃ©mantique, 8 heures seront passÃ©es sur la syntaxe des commentaires)

Similaire Ã  la [loi de trivialitÃ©](#loi-de-futilite), la loi de Wadler Ã©nonce que lors de la conception d'un langage, le temps passÃ© Ã  discuter des diffÃ©rents aspects est inversement proportionnel Ã  l'importance de ces aspects.

Voir aussi :

- [Loi de futilitÃ©](#loi-de-futilite)

### Loi de Wheaton

[Le lien](http://www.wheatonslaw.com/)

[Le jour officiel](https://dontbeadickday.com/)

> Ne soyez pas un connard.
> *Wil Wheaton*

InventÃ©e par Will Wheaton (Star Trek: The Next Generation, The Big Bang Theory), cette loi simple, concise et puissante vise Ã  augmenter l'harmonie et le respect au sein d'un environnement professionnel. Elle peut Ãªtre appliquÃ©e lorsqu'on parle Ã  ses collÃ¨gues, effectue une code review, argumente contre un autre point de vue, critique et de maniÃ¨re gÃ©nÃ©rale, lors de la plupart des interactions entre humains.

## Principes

Les principes sont gÃ©nÃ©ralement des lignes directrices liÃ©s Ã  la conception.

### Principe de Dilbert

[Principe de Dilbert sur Wikipedia](https://fr.wikipedia.org/wiki/Principe_de_Dilbert)

> Les entreprises tendent Ã  promouvoir systÃ©matiquement les employÃ©s incompÃ©tents afin de les sortir du workflow.
> *Scott Adams*

Un concept de gestion inventÃ© par Scott Adams (crÃ©ateur du comic strip Dilbert) inspirÃ© par le [principe de Peter](#principe-de-peter). Suivant le principe de Dilbert, les employÃ©s qui n'ont jamais montrÃ© de compÃ©tence dans leur travail sont promus Ã  des postes de management afin de limitÃ© les dommages qu'ils peuvent causer. Adams expliqua initialement le principe dans un article du Wall Street Journal datant de 1995, et Ã©labora le concept dans son livre de 1996: [The Dilbert Principle](#a-lire).

Voir aussi :

- [Principe de Peter](#principe-de-peter)
- [Loi de Putt](#loi-de-putt)

### Principe de Pareto (rÃ¨gle des 80/20)

[Principe de Pareto sur Wikipedia](https://fr.wikipedia.org/wiki/Principe_de_Pareto)

> La plupart des choses dans la vie ne sont pas rÃ©parties Ã©galement.

Le principe de Pareto suggÃ¨re que dans certains cas, la majoritÃ© des rÃ©sultats provient d'une minoritÃ© des actions :

- 80% d'un certain logiciel peut Ãªtre Ã©crit en 20% du temps de dÃ©veloppement allouÃ© (inversement, les 20% les plus difficiles prennent 80% du temps)
- 20% de l'effort fourni produit 80% du rÃ©sultat
- 20% du travail amÃ¨ne 80% des revenus
- 20% des bugs causent 80% des crashs
- 20% des fonctionnalitÃ©s entrainent 80% de l'utilisation

Dans les annÃ©es 1940, l'ingÃ©nieur AmÃ©ricano-Roumain Dr. Joseph Juran, qui est largement crÃ©ditÃ© comme Ã©tant le pÃ¨re du contrÃ´le qualitÃ©, [commenÃ§a Ã  appliquer le principe de Pareto pour rÃ©soudre des problÃ¨mes de qualitÃ©](https://en.wikipedia.org/wiki/Joseph_M._Juran).

Ce principe est aussi connu comme la rÃ¨gle des 80/20, 'The Law of the Vital Few' et 'The Principle of Factor Sparsity'.

Exemples concrets :

- - En 2002, Microsoft reporta qu'en rÃ©glant les 20% des bugs les plus reportÃ©s, 80% des erreurs et des crashs liÃ©s dans Windows et Office ont Ã©tÃ© Ã©liminÃ©s ([Reference](https://www.crn.com/news/security/18821726/microsofts-ceo-80-20-rule-applies-to-bugs-not-just-features.htm)).

### Principe de Peter

[Principe de Peter sur Wikipedia](https://en.wikipedia.org/wiki/Peter_principle)

> Les gens faisant partie d'une hiÃ©rarchie tendent Ã  s'Ã©lever Ã  leur "niveau d'incompÃ©tence"
> *Laurence J. Peter*

Le principe de Peter est un concept de management inventÃ© par Laurence J. Peter qui observe que les gens qui sont bons dans leur travail sont promus jusqu'Ã  ce qu'ils atteignent un niveau oÃ¹ ils ne rÃ©ussissent plus (leur "niveau d'incompÃ©tence"). Ã€ ce point, Ã©tant donnÃ© leur expÃ©rience ils sont moins susceptibles de se faire renvoyer (Ã  part s'ils obtiennent des rÃ©sultats particuliÃ¨rement mauvais) et vont demeurer dans un poste pour lequel ils ont potentiellement peu de compÃ©tences.

Ce principe est particuliÃ¨rement intÃ©ressant pour les ingÃ©nieurs qui dÃ©marrent leur carriÃ¨re dans des postes profondÃ©ment techniques mais Ã©voluent souvent vers des postes de *managers*, qui requiert des compÃ©tences fondamentalement diffÃ©rentes.

Voir aussi :

- [Principe de Dilbert](#principe-de-dilbert)
- [Loi de Putt](#loi-de-putt)

### Principe de robustesse (loi de Postel)

[Principe de robustesse sur Wikipedia](https://fr.wikipedia.org/wiki/Jon_Postel#Principe_de_robustesse)

> Soyez tolÃ©rant dans ce que vous acceptez, et pointilleux dans ce que vous envoyez

Souvent appliquÃ© dans le dÃ©veloppement d'application serveur, ce principe Ã©nonce que ce que vous envoyez aux autres devrait Ãªtre aussi minimal et conforme que possible. Mais que vous devriez accepter des donnÃ©es en entrÃ©e non-conforme si elles peuvent Ãªtre traitÃ©s.

Le but de ce principe est de construire des systÃ¨mes qui sont robustes dans le sens oÃ¹ ils peuvent gÃ©rer des entrÃ©es mal formÃ©es du moment qu'elles restent comprÃ©hensibles. Cependant, il y a de potentielles implications de sÃ©curitÃ© Ã  accepter des entrÃ©s mal formÃ©es. ParticuliÃ¨rement si le traitement de ces entrÃ©es n'est pas correctement testÃ©.

Ã€ terme, autoriser des entrÃ©es non-conforme peut amoindrir la capacitÃ© d'Ã©volution des protocoles Ã©tant donnÃ© que les utilisateurs vont tÃ´t ou tard compter sur cette tolÃ©rance lors de leur utilisation.

Voir aussi :

- [Loi d'Hyrum](#loi-dhyrum)

### SOLID

Il s'agit d'un acronyme qui signifie :

- S: [Single responsibility principle](#principe-de-responsabilite-unique) (principe de responsabilitÃ© unique)
- O: [The Open/Closed Principle](#principe-ouvertferme) (principe ouvert/fermÃ©)
- L: [The Liskov Substitution Principle](#principe-de-substitution-de-liskov) (Principe de substitution de Liskov)
- I: [The Interface Segregation Principle](#principe-de-segregation-des-interfaces) (principe de sÃ©grÃ©gation des interfaces)
- D: [Principe d'inversion des dÃ©pendances](#principe-dinversion-des-dependances)

Ces principes sont fondamentaux dans la [programmation orientÃ©e objet](#TODO). Ces principes de conception devraient pouvoir aider les dÃ©veloppeurs Ã  construire des systÃ¨mes plus facilement maintenable.

### Principe de responsabilitÃ© unique

[Principe de responsabilitÃ© unique sur Wikipedia](https://en.wikipedia.org/wiki/Single_responsibility_principle)

> Chaque module ou classe ne doit avoir qu'une seule responsabilitÃ©.

Le premier des principes '[SOLID](#solid)'. Il suggÃ¨re que les modules ou classes ne devraient faire qu'une chose unique. Autrement dit, un seul petit changement sur une fonctionnalitÃ© d'un programme ne devrait nÃ©cessiter de changer qu'un seul composant. Par exemple, changer la maniÃ¨re de valider un mot de passe ne devrait nÃ©cessiter un changement qu'Ã  un endroit du programme.

ThÃ©oriquement, cela devrait rendre le code plus robuste et plus simple Ã  modifier. Savoir qu'un composant en train d'Ãªtre modifiÃ© possÃ¨de une seule responsabilitÃ© veut aussi dire que *tester* cette modification devrait Ãªtre plus simple. Pour reprendre l'exemple precedent, changer le composant concernant la validation d'un mot de passe ne devrait affecter que cette fonctionnalitÃ©. Il est souvent beaucoup plus difficile de rÃ©flÃ©chir aux impacts d'un changement sur un composant qui est responsable de plusieurs choses.

Voir aussi :

- [Programmation orientÃ©e objet](#todo)
- [SOLID](#solid)

### Principe ouvert/fermÃ©

[Principe ouvert/fermÃ© sur Wikipedia](https://fr.wikipedia.org/wiki/Principe_ouvert/ferm%C3%A9)

> Les entitÃ©s devraient Ãªtre ouvertes Ã  l'extension et fermÃ©es Ã  la modification.

Le deuxiÃ¨me des principes '[SOLID](#solid)'. Il Ã©nonce que le comportement des entitÃ©s (classes, modules, fonctions, etc.) devraient pouvoir Ãªtre *Ã©tendu*, mais que le comportement *existant* ne devrait pas pouvoir Ãªtre modifiÃ©.

Imaginons par exemple un module capable de changer un document rÃ©digÃ© en Markdown en HTML. Ce module peut Ãªtre Ã©tendu en y ajoutant le support pour une nouvelle fonctionnalitÃ© Markdown sans modifier son fonctionnement interne. Le module est en revanche *fermÃ©* Ã  la modification dans le sens oÃ¹ un utilisateur *ne peut pas* changer la maniÃ¨re dont le code existant est rÃ©digÃ©.

Ce principe est particuliÃ¨rement pertinent pour la programmation orientÃ©e objet, oÃ¹ on cherche la plupart du temps Ã  concevoir des objets qu'on puisse facilement Ã©tendre mais dont le comportement existant ne puisse pas Ãªtre modifiÃ© de maniÃ¨re imprÃ©vue.

Voir aussi :

- [Programmation orientÃ©e objet](#todo)
- [SOLID](#solid)

### Principe de substitution de Liskov

[Principe de substitution de Liskov sur Wikipedia](https://fr.wikipedia.org/wiki/Principe_de_substitution_de_Liskov)

> Il devrait Ãªtre possible de remplacer un type avec un sous-type sans casser le systÃ¨me.

Le troisiÃ¨me des principes '[SOLID](#solid)'. Il Ã©nonce que si un composant repose sur un type, alors il devrait Ãªtre capable d'utiliser un sous-type de ce type sans que le systÃ¨me ne plante ou qu'il y ai besoin de connaitre les dÃ©tails de ce sous-type.

Imaginons par exemple que nous ayons une mÃ©thode qui lit un document XML depuis une structure reprÃ©sentant un fichier. Si cette mÃ©thode utilise un type 'fichier' de base, alors tous les types dÃ©rivant de 'fichier' devraient pouvoir Ãªtre utilisÃ© avec cette fonction. Si 'fichier' supporte une recherche partant de la fin et que le parser XML utilise cette fonction, mais que le type dÃ©rivÃ© 'fichier rÃ©seau' ne permet pas de recherche en partant de la fin, alors 'fichier rÃ©seau' viole le principe.

Ce principe est particuliÃ¨rement pertinent pour la programmation orientÃ©e objet, oÃ¹ les hierarchies de types doivent Ãªtre conÃ§ues soigneusement pour Ã©viter de brouiller les utilisateurs d'un systÃ¨me.

Voir aussi :

- [Programmation orientÃ©e objet](#todo)
- [SOLID](#solid)

### Principe de sÃ©grÃ©gation des interfaces

[Principe de sÃ©grÃ©gation des interfaces sur Wikipedia](https://fr.wikipedia.org/wiki/Principe_de_s%C3%A9gr%C3%A9gation_des_interfaces)

> Aucun client de devrait dÃ©pendre de mÃ©thodes qu'il n'utilise pas.

Le quatriÃ¨me des principes '[SOLID](#solid)'. Celui-ci Ã©nonce que les utilisateurs d'un composant ne devraient pas dÃ©pendre des fonctions de ce composant qu'il n'utilise pas.

Par exemple, imaginons que nous ayons une mÃ©thode qui lit un document XML depuis une structure reprÃ©sentant un fichier. Elle nÃ©cÃ©ssite seulement de pouvoir lire des octets, avancer ou reculer dans le fichier. Le principe sera invalidÃ© si cette mÃ©thode a besoin d'Ãªtre mise Ã  jour lorsqu'une fonctionnalitÃ© sans rapport du fichier change (ex. une mise Ã  jour de modÃ¨le de permissions pour l'accÃ¨s au fichier). Il serait prÃ©fÃ©rable pour le fichier d'implÃ©menter une interface 'seekable-stream', et pour le lecteur XML de l'utiliser.

Ce principe est particuliÃ¨rement pertinent pour la programmation orientÃ©e objet, oÃ¹ les interfaces, hierarchies et type abstraits sont utilisÃ©s pour [minimiser le couplage](#todo) entre les diffÃ©rents composants. Le [duck typing](#todo) est une mÃ©thode qui applique ce principe en Ã©liminant les interfaces explicites.

Voir aussi :

- [Programmation orientÃ©e objet](#todo)
- [SOLID](#solid)
- [Duck Typing](#todo)
- [Decouplage](#todo)

### Principe d'inversion des dÃ©pendances

[Principe d'inversion des dÃ©pendances sur Wikipedia](https://fr.wikipedia.org/wiki/Inversion_des_d%C3%A9pendances)

> Les modules de haut niveau ne devraient pas dÃ©pendre des implÃ©mentations de bas niveau.

Le cinquiÃ¨me des principes '[SOLID](#solid)'. Il Ã©nonce que les composants de hauts niveau ne devraient pas avoir Ã  connaitre les dÃ©tails de leurs dependences.

Prenons par exemple un programme qui lit des mÃ©ta-donnÃ©s depuis un site web. Ce programme possÃ¨de un composant principal qui connait un autre composant chargÃ© de tÃ©lÃ©charger le contenu de la page, ainsi qu'un autre composant capable de lire les mÃ©ta-donnÃ©s. En prenant en compte le principe d'inversion des dÃ©pendances, le composant principal ne devrait dÃ©pendre que de: un composant abstrait capable de tÃ©lÃ©charger des donnÃ©es binaires, ainsi que d'un composant abstrait capable de lire des mÃ©ta-donnÃ©s depuis un flux binaire. Le composant principal ne devrais pas Ã  connaitre les concepts de TCP/IP, HTTP, HTML, etc.

Ce principe est complexe Ã©tant donnÃ© qu'il semble 'inverser' les dÃ©pendances attendues dans un systÃ¨me (d'oÃ¹ le nom). En pratique cela veut aussi dire qu'un composant 'orchestrateur' doit s'assurer que les types abstraits soient correctement implÃ©mentÃ©s. (Pour reprendre l'exemple prÃ©cÃ©dent, *quelque chose* doit fournir un downloader de fichier HTTP et un liseur de meta tag HTML au composant lisant les mÃ©ta-donnÃ©s.) On touche alors Ã  des patterns tels que l'[inversion de contrÃ´le](#todo) et l'[injection de dÃ©pendances](#todo).

Voir aussi :

- [Programmation orientÃ©e objet](#todo)
- [SOLID](#solid)
- [Inversion de contrÃ´le](#todo)
- [Injection de dÃ©pendances](#todo)

### Principe DRY

[Principe DRY sur Wikipedia](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)

> Dans un systÃ¨me, toute connaissance doit avoir une reprÃ©sentation unique, non-ambiguÃ«, faisant autoritÃ©.

DRY est un acronyme pour *Don't Repeat Yourself* (ne vous rÃ©pÃ©tez pas). Ce principe vise Ã  aider les dÃ©veloppeurs Ã  rÃ©duire les rÃ©pÃ©titions de code et Ã  garder l'information Ã  un seul endroit. Il a Ã©tÃ© formulÃ© en 1999 par Andrew Hunt et Dave Thomas dans le livre [The Pragmatic Developer](https://en.wikipedia.org/wiki/The_Pragmatic_Programmer).

> L'opposÃ© de DRY serait *WET* (Write Everything Twice ou We Enjoy Typing, qu'on peut traduire par Tout Ã©crire en double ou On aime taper au clavier).

En pratique, si vous avez la mÃªme information dans deux (ou plus) endroits, vous pouvez utiliser DRY pour les fusionner et rÃ©utiliser cette unique instance partout oÃ¹ c'est nÃ©cessaire.

Voir aussi :

- [The Pragmatic Developer](https://en.wikipedia.org/wiki/The_Pragmatic_Programmer)

### Principe KISS

[KISS sur Wikipedia](https://fr.wikipedia.org/wiki/Principe_KISS)

> > Keep it simple, stupid. (Ne complique pas les choses)

Le principe KISS Ã©nonce que la plupart des systÃ¨mes fonctionnent mieux s'ils sont simples que compliquÃ©s. Par consÃ©quent, la simplicitÃ© devrait Ãªtre un but essentiel dans la conception et toute complexitÃ© inutile devrait Ãªtre Ã©vitÃ©. Provenant de la marine AmÃ©ricaine en 1960, la phrase est attribuÃ©e Ã  l'ingÃ©nieur Kelly Johnson.

Le principe est exemplifiÃ© le mieux par l'histoire de Johnson qui donna Ã  une Ã©quipe d'ingÃ©nieurs une poignÃ©e d'outils et le dÃ©fi de concevoir un avion de chasse qui soit rÃ©parable par un mÃ©canicien lambda, sur le terrain, en condition de combat avec ces outils uniquement. Le "supid" fait donc rÃ©fÃ©rence Ã  la relation entre la maniÃ¨re dont les choses cassent et la sophistication des outils Ã  disposition pour les rÃ©parer, et non aux capacitÃ©s des ingÃ©nieurs eux-mÃªmes.

Voir aussi :

- [Loi de Gall](#loi-de-gall)

### YAGNI

[YAGNI sur Wikipedia](https://fr.wikipedia.org/wiki/YAGNI)

Il s'agit d'un acronyme pour ***Y**ou **A**in't **G**onna **N**eed **I**t*. Que l'on peut traduire par: "vous n'en aurez pas besoin".

> ImplÃ©mentez les choses uniquement quand vous en avez rÃ©ellement besoin et non quand vous pensez que vous en aurez besoin plus tard.
> ([Ron Jeffries](https://twitter.com/RonJeffries)) (Co-fondateur de XP et auteur du livre "Extreme Programming Installed")

Ce principe *d'Extreme Programming* (XP) suggÃ¨re que les dÃ©veloppeurs ne devraient implÃ©menter que les fonctionnalitÃ©s qui sont nÃ©cessaires immÃ©diatement et Ã©viter de tenter de prÃ©dire l'avenir en implÃ©mentant des fonctionnalitÃ©s qui pourraient Ãªtre nÃ©cessaires plus tard.

AdhÃ©rer Ã  ce principe devrait rÃ©duire la quantitÃ© de code inutilisÃ© dans la codebase et permettre d'Ã©viter de passer du temps et des efforts sur des fonctionnalitÃ©s qui n'apportent rien.

Voir aussi :

- [Ã€ lire: Extreme Programming Installed](#a-lire)

### Illusions de l'informatique distribuÃ©e

[Illusions de l'informatique distribuÃ©e sur Wikipedia](https://fr.wikipedia.org/wiki/Illusions_de_l%27informatique_distribu%C3%A9e)

Aussi connues sous le nom de *illusions de l'informatique en rÃ©seau*, les illusions sont une liste de suppositions (ou croyances) concernant l'informatique distribuÃ©e, qui peuvent amener Ã  des problÃ¨mes dans le dÃ©veloppement logiciel. Les suppositions sont :

- Le rÃ©seau est fiable
- Le temps de latence est nul
- La bande passante est infinie
- Le rÃ©seau est sÃ»r
- La topologie du rÃ©seau ne change pas
- Il y a un et un seul administrateur rÃ©seau
- Le coÃ»t de transport est nul
- Le rÃ©seau est homogÃ¨ne

Les quatre premiers Ã©lÃ©ments furent listÃ©s par [Bill Joy](https://en.wikipedia.org/wiki/Bill_Joy) et [Tom Lyon](https://twitter.com/aka_pugs) vers 1991 et qualifiÃ©s pour la premiÃ¨re fois d'"illusions de l'informatique distribuÃ©e" par [James Gosling](https://en.wikipedia.org/wiki/James_Gosling). [L. Peter Deutsch](https://en.wikipedia.org/wiki/L._Peter_Deutsch) ajouta les 5Ã¨me, 6Ã¨me et 7Ã¨me illusions. Gosling ajouta la 8Ã¨me illusion vers la fin des annÃ©es 90.

Le groupe Ã©tait inspirÃ© par ce qui se passait Ã  l'Ã©poque chez [Sun Microsystems](https://en.wikipedia.org/wiki/Sun_Microsystems).

Ces illusions devraient Ãªtre gardÃ©es Ã  l'esprit pour concevoir du code rÃ©sistant Ã©tant donnÃ© que chacune d'entre elle peut mener Ã  une perception biaisÃ©e qui ne prend pas en compte la rÃ©alitÃ© et la complexitÃ© des systÃ¨mes distribuÃ©s.

Voir aussi :

- [Foraging for the Fallacies of Distributed Computing (Part 1) - Vaidehi Joshi on Medium](https://medium.com/baseds/foraging-for-the-fallacies-of-distributed-computing-part-1-1b35c3b85b53)
- [Deutsch's Fallacies, 10 Years After](http://java.sys-con.com/node/38665)

## Ã€ lire

Si vous avez trouvÃ© ces concepts intÃ©ressants, vous apprÃ©cierez peut Ãªtre aussi les livres suivants :

- [Extreme Programming Installed - Ron Jeffries, Ann Anderson, Chet Hendrikson](https://www.goodreads.com/en/book/show/67834) - Couvre les principes fondamentaux de l'Extreme Programming.
- [The Mythical Man Month - Frederick P. Brooks Jr.](https://www.goodreads.com/book/show/13629.The_Mythical_Man_Month) - Un classique sur le dÃ©veloppement logiciel. La [loi de Brooks](#loi-de-brooks) est un thÃ¨me central du livre.
- [GÃ¶del, Escher, Bach: An Eternal Golden Braid - Douglas R. Hofstadter.](https://www.goodreads.com/book/show/24113.G_del_Escher_Bach) - Un livre difficile Ã  classe. La [loi de Hofstadter](#loi-de-hofstadter) est tirÃ©e de ce livre.
- [The Dilbert Principle - Scott Adams](https://www.goodreads.com/book/show/85574.The_Dilbert_Principle) - Un regard humoristique sur l'AmÃ©rique corporate, par l'auteur du [principle de Dilbert](#principe-de-dilbert).
- [The Peter Principle - Lawrence J. Peter](https://www.goodreads.com/book/show/890728.The_Peter_Principle) - Un autre regard humoristique portÃ© sur les challenges du management et des grandes entreprises. L'origine du [principe de Peter](#principe-de-peter).

## Traductions

GrÃ¢ce Ã  l'aide de merveilleux contributeurs, Hacker Laws est disponible dans plusieurs langues. N'hÃ©sitez pas Ã  envisager de sponsoriser les modÃ©rateurs !

Langue | Moderateur | Status
--- | --- | ---
[ğŸ‡§ğŸ‡· Brasileiro / BrÃ©silien](./translations/pt-BR.md) | [Leonardo Costa](https://github.com/leofc97) | [![gitlocalized ](https://gitlocalize.com/repo/2513/pt-BR/badge.svg)](https://gitlocalize.com/repo/2513/pt-BR?utm_source=badge)[](https://gitlocalize.com/repo/2513/pt-BR?utm_source=badge)[](https://gitlocalize.com/repo/2513/pt-BR?utm_source=badge)
[ğŸ‡¨ğŸ‡³ ä¸­æ–‡ / Chinois](https://github.com/nusr/hacker-laws-zh) | [Steve Xu](https://github.com/nusr) | Partiellement complÃ¨te
[ğŸ‡©ğŸ‡ª Deutsch / Allemand](./translations/de.md) | [Vikto](https://github.com/viktodergunov) | [![gitlocalized ](https://gitlocalize.com/repo/2513/de/badge.svg)](https://gitlocalize.com/repo/2513/de?utm_source=badge)[](https://gitlocalize.com/repo/2513/de?utm_source=badge)[](https://gitlocalize.com/repo/2513/de?utm_source=badge)
[ğŸ‡«ğŸ‡· FranÃ§ais / French](./translations/fr.md) | [Kevin Bockelandt](https://github.com/KevinBockelandt) | [![gitlocalized ](https://gitlocalize.com/repo/2513/fr/badge.svg)](https://gitlocalize.com/repo/2513/fr?utm_source=badge)[](https://gitlocalize.com/repo/2513/fr?utm_source=badge)[](https://gitlocalize.com/repo/2513/fr?utm_source=badge)
[ğŸ‡¬ğŸ‡· ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬ / Grecque](./translations/el.md) | [Panagiotis Gourgaris](https://github.com/0gap) | [![gitlocalized ](https://gitlocalize.com/repo/2513/el/badge.svg)](https://gitlocalize.com/repo/2513/el?utm_source=badge)[](https://gitlocalize.com/repo/2513/el?utm_source=badge)[](https://gitlocalize.com/repo/2513/el?utm_source=badge)
[ğŸ‡®ğŸ‡¹ Italiano / Italien](https://github.com/csparpa/hacker-laws-it) | [Claudio Sparpaglione](https://github.com/csparpa) | Partiellement complÃ¨te
[ğŸ‡°ğŸ‡· í•œêµ­ì–´ / CorÃ©en](https://github.com/codeanddonuts/hacker-laws-kr) | [Doughnut](https://github.com/codeanddonuts) | Partiellement complÃ¨te
[ğŸ‡±ğŸ‡» LatvieÅ¡u Valoda / Letton](./translations/lv.md) | [Arturs Jansons](https://github.com/iegik) | [![gitlocalized ](https://gitlocalize.com/repo/2513/lv/badge.svg)](https://gitlocalize.com/repo/2513/lv?utm_source=badge)[](https://gitlocalize.com/repo/2513/lv?utm_source=badge)[](https://gitlocalize.com/repo/2513/lv?utm_source=badge)
[ğŸ‡·ğŸ‡º Ğ ÑƒÑÑĞºĞ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ / Russe](https://github.com/solarrust/hacker-laws) | [Alena Batitskaya](https://github.com/solarrust) | Partiellement complÃ¨te
[ğŸ‡ªğŸ‡¸ Castellano / Espagnol](./translations/es-ES.md) | [Manuel Rubio](https://github.com/manuel-rubio) ([Sponsor](https://github.com/sponsors/manuel-rubio)) | Partiellement complÃ¨te
[ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e / Turc](https://github.com/umutphp/hacker-laws-tr) | [Umut IÅŸÄ±k](https://github.com/umutphp) | [![gitlocalized ](https://gitlocalize.com/repo/2513/tr/badge.svg)](https://gitlocalize.com/repo/2513/tr?utm_source=badge)[](https://gitlocalize.com/repo/2513/tr?utm_source=badge)[](https://gitlocalize.com/repo/2513/tr?utm_source=badge)

Si vous souhaitez mettre Ã  jour une traduction, vous pouvez [ouvrir une pull request](https://github.com/dwmkerr/hacker-laws/pulls). Si vous voulez ajouter une nouvelle langue, connectez vous Ã  [GitLocalize](https://gitlocalize.com/) pour crÃ©er un compte, puis crÃ©ez une issue afin que je vous ajoute au projet ! Il serait Ã©galement trÃ¨s apprÃ©ciÃ© d'ouvrir une pull request correspondante qui met Ã  jour le tableau ci-dessus et la liste de liens au dÃ©but de ce fichier.

## Projets liÃ©s

- [Tip of the Day](https://tips.darekkay.com/html/hacker-laws-en.html) - Recevez quotidiennement une loi / principe.

## Contribuer

N'hÃ©sitez pas Ã  contribuer ! Vous pouvez [crÃ©er une issue](https://github.com/dwmkerr/hacker-laws/issues/new) pour suggÃ©rer une addition ou un changement, ou [ouvrir une pull request](https://github.com/dwmkerr/hacker-laws/compare) pour proposer vos propres modifications.

Merci de lire le [guide de contribution](./.github/contributing.md) pour connaitre les prÃ©-requis concernant le style, le contenu, etc. Veuillez lire Ã©galement le [code de conduite](./.github/CODE_OF_CONDUCT.md) afin de le respecter lors des discussions sur le projet.

## TODO

Si vous Ãªtes atteris ici vous avez cliquÃ© sur un lien concernant un sujet qui n'a pas encore Ã©tÃ© rÃ©digÃ©. DÃ©solÃ© ! Tout n'est pas encore terminÃ©.

N'hÃ©sitez pas Ã  [crÃ©er une issue](https://github.com/dwmkerr/hacker-laws/issues) pour avoir plus de dÃ©tails, ou [ouvrez une pull request](https://github.com/dwmkerr/hacker-laws/pulls) pour soumettre votre propre texte sur le sujet.
